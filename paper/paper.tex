\documentclass{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{lmodern}        % https://github.com/rstudio/rticles/issues/343
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}

\title{Automatically drawing vegetation maps using digital time-lapse cameras in alpine ecosystems}

\author{
    Ryotaro Okamoto
    \thanks{\url{https://github.com/0kam}}
   \\
    Doctoral Program in Biology \\
    University of Tsukuba \\
  1-1-1 Tennodai, Tsukuba, Ibaraki 305-8577 Japan. \\
  \texttt{\href{mailto:okamoto.ryotaro.su@alumni.tsukuba.ac.jp}{\nolinkurl{okamoto.ryotaro.su@alumni.tsukuba.ac.jp}}} \\
   \And
    Hiroyuki Oguma
   \\
    Biodiversity Division \\
    National Institute for Environmental Studies \\
  16-2 Onogawa, Tsukuba, Ibaraki 305-8506 JAPAN \\
  \texttt{\href{mailto:oguma@nies.go.jp}{\nolinkurl{oguma@nies.go.jp}}} \\
   \And
    Reiko Ide
   \\
    Earth System Division \\
    National Institute for Environmental Studies \\
  16-2 Onogawa, Tsukuba, Ibaraki 305-8506 JAPAN \\
  \texttt{\href{mailto:ide.reiko@nies.go.jp}{\nolinkurl{ide.reiko@nies.go.jp}}} \\
  }


% tightlist command for lists without linebreak
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% From pandoc table feature
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}


\usepackage{amsmath}
\setlength{\parindent}{10.5pt}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\begin{document}
\maketitle


\begin{abstract}
Enter the text of your abstract here.
\end{abstract}

\keywords{
    alpine ecology
   \and
    deep learning
   \and
    ecosystem monitoring
  }

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

The effects of climate change on terrestrial ecosystems are particularly significant in alpine regions (\cite{IPCC2007}). Alpine vegetation depends on severe conditions such as low temperatures and long snow-covered periods. Thus alpine areas have rare and unique species adapted to the extreme environments. Several studies have reported that recent global climatic changes, e.g., increasing temperatures and reducing snow-covered periods, have accelerated the invasion of non-native species into alpine areas (see \cite{Alexander2016AlpBotany}). In Japan, dwarf bamboo (\emph{Sasa kurilensis}) has invaded alpine snow meadows, probably driven by the extension of the snow-free period (\cite{Kudo2011EcoEvo}). Also, climate change has affected the growth and phenologies of native species. For example, the growth of dwarf pine (\emph{Pinus pumila}), a dominant species in Japanese alpine regions, has been affected by climatic conditions such as temperature and snowmelt (\cite{Amagai2015EcoRes}). Monitoring and predicting such changes are essential for effective conservation planning. Since the impact of climate change on alpine vegetation varies depending on species and the microhabitats (\cite{Kudo2010AAA}), spatially high-resolution monitoring with a wide range is required.

Previous studies have mainly depended on field observations, yet it is hard to cover broad areas in alpine regions due to poor accessibility and severe weather. Satellite, airborne, and Unmanned Aerial Vehicle (UAV) remote sensing methods seem to be alternatives. However, satellite imagery of alpine areas is rarely available due to cloud cover, and the spatial resolution is not enough to observe vegetation changes at the plant community scale. Airborne imagery can obtain high-resolution data, but its cost becomes a bottleneck for frequent monitoring. Although UAV methods have become a cost-effective tool for ecological monitoring (\cite{Baena2017PLOSONE}), operating UAVs in alpine regions is challenging due to the strong wind and harsh topology.

On the other hand, researchers have also utilized automated digital time-lapse cameras mounted on the ground for monitoring green-leaf phenologies in forests (\cite{Richardson2009EcolAppl}), grasslands (\cite{Browning2017RemSen}), and alpine meadows (\cite{IdeOguma2013EcolInfom}). Unlike satellite imagery, such cameras provide images free of clouds and atmospheric effects. Also, they can obtain high-resolution (i.e., sub-meter scale) and frequent (i.e., daily or hourly) images at a meager cost. These studies set some regions of interest (ROI) in the images and calculate the phenology index (e.g., excess greenness, \cite{Woebbecke1995ASAE}). Also, we can visually interpret the vegetation distribution and its changes from such time-lapse images. However, few studies have tackled this. This lack of studies seems to be because applying such time-lapse imagery in monitoring vegetation distribution has two technical challenges.

First, unlike satellites' multispectral sensors, ordinal digital cameras can only obtain three bands (Red, Green, and Blue), making it harder to classify the vegetation. Second, since digital time-lapse cameras are mounted on the ground, transforming these images into geospatial data (e.g., orthoimage) is challenging. In other words, even if we find a vegetation change, we cannot identify its geographic location and analyze it with other geographic data (e.g., topography). Treating ground-based time-lapse images as geographic data is essential to utilize them in conservation planning.

This study proposes an automated method for drawing vegetation maps and locating vegetation changes with a digital time-lapse camera by solving these two challenges. We solved the first challenge by utilizing temporal changes in leaf colors in vegetation classification. We also developed an accurate method for transforming a ground-based photograph into geographic data to tackle the second challenge. Finally, we show an example application of our method by detecting and localizing vegetation changes with an 8-years digital time-lapse imagery of the Japanese alps. We aim to use cheap but powerful digital time-lapse cameras in alpine ecosystem conservation.

\hypertarget{materials-and-methods}{%
\section{Materials and methods}\label{materials-and-methods}}

\hypertarget{digital-time-lapse-camera-imagery}{%
\subsection{Digital time-lapse camera imagery}\label{digital-time-lapse-camera-imagery}}

We used repeat photography data owned by National Institute for Environmental Studies, Japan (NIES). All the images are publically available on NIES' webpage (\url{https://db.cger.nies.go.jp/gem/ja/mountain/station.html?id=2}). In 2010, NIES installed the digital time-lapse camera (EOS 5D MK2, Canon Inc., 21 M pixels) on a mountain lodge Murodo-sanso (about 2350 m a.s.l., above the forest limit), located at the foot of Mt. Tateyama (3015 m a.s.l.), in the Nothern Japanese Alps. The camera takes one photograph per hour, from 6 a.m. to 7 p.m. . The camera's field of view (FOV) includes Mt. Tateyama, which ranges from about 2350 m a.s.l. to 3015 m a.s.l. in elevation. The area has a complex mosaic-like vegetation structure because of its topography, including rocks, cliffs, curls, and moraines. From April to November, the camera has observed the snowmelt and seasonal phenology of evergreen, deciduous dwarf trees (e.g., \emph{Pinus pumila}, \emph{Sorbus} sp.), dwarf bamboos (e.g., \emph{Sasa kurilensis}), and alpine herbaceous plants (e.g., \emph{Geum pentapetalum}, \emph{Nephrophyllidium crista-galli}). We used the images from late summer to late fall of 2012 and 2020.

\hypertarget{preprocessing}{%
\subsection{Preprocessing}\label{preprocessing}}

\hypertarget{selecting-images}{%
\subsubsection{Selecting images}\label{selecting-images}}

First, we selected images that are suitable for vegetation classification. We choose four days with good weather from late summer to late fall each year (9/19, 10/1, 10/11, 10/23 in 2010. 9/19, 10/2, 10/11, 10/23 in 2020). We used the images of this season because we can separate vegetation from the patterns of autumn foliage coloration. Also, we chose four images from 11 a.m. to 2 p.m. each day to avoid the effect of temporal noise such as shadows. Finally, we got 16 images for each year.

\hypertarget{automatic-image-to-image-alignment}{%
\subsubsection{Automatic image-to-image alignment}\label{automatic-image-to-image-alignment}}

Since images are slightly misaligned with each other due to wind, we aligned them before processing. We implemented the program with Python3 programming language and OpenCV4 (\url{https://opencv.org/}) image processing library. First, we set one image of 2010 as the alignment target. Next, we automatically found matching keypoints between the target and other images using AKAZE local feature extractor (\cite{Alcantarilla2013AKAZE}) and K-nearest neighbor matcher. Then, we searched and applied the homography matrix that minimizes the distance between each pair of the matching points, using OpenCV's ``findHomography'' function.

\hypertarget{automatic-vegetation-classification}{%
\subsection{Automatic vegetation classification}\label{automatic-vegetation-classification}}

Next, we classified the pixel time series of repeat photography imagery into vegetation. Because deciduous plants have great differences in autumn phenology among species, researchers have used that information for vegetation classification with satellite imagery (e.g., \cite{Tigges2013RemSenEnv}, \cite{Son2013RemSen}, \cite{Heupel2018PFG}). In our target area, we can recognize the vegetation type with the pixel time series of autumn (Fig. 2). However, no research has applied this technique to ground-based repeat photography imagery. We developed a deep-learning method to take advantage of high-resolution and frequent repeat photography data. Fig.3 shows the classification process.

\hypertarget{model-architecture}{%
\subsubsection{Model Architecture}\label{model-architecture}}

Since ground-based photographs can provide high spatial resolution (sub-meter scale in our dataset), we can use leaf texture as additional information for classification. For example, we can see a glossy surface on dwarf bamboos and a mat surface on dwarf pines. Thus, we used a small patch (9x9 pixels square) rather than a single pixel as an input of the model. Therefore, a model input is a time series of image patches.

Our model has two components to deal with this data structure (see Fig. 3). First, we extracted each patch's features (e.g., texture) using Convolutional Neural Network (CNN) layers (Fig. 4a). CNN is a neural network specialized in recognizing spatial structures of data, such as images. The CNN part outputs a time series of extracted features. Second, we used Recurrent Neural Net (RNN) layers (Fig. 4b) to classify the temporal patterns of the features extracted by the CNN part. RNN is a neural network specialized in recognizing temporal or sequential data dynamics, such as text and speech. Amongst many variants of RNN, we used Long Short Time Memory (LSTM, \cite{Hochreiter1997LSTM}). Combining CNN and RNN, our model can classify the input image-patch time series considering the colors and textures of each patch and their temporal patterns. This CNN-RNN architecture has also been used for audio classification(\cite{Shi2015CRNNtext}) and scene text recognition(\cite{Shi2015CRNNtext}).

Using a Deep Learning method has another positive side effect; mini-batch training. Since we used high-resolution images (16 x 21M pix), the training dataset became so immense that we could not feed them to the model at once: that consumes the computer's RAM too much. However, using Deep Learning methods enables us to provide them in small portions (called mini-batches) to save RAM consumption.

\hypertarget{dataset-preparation}{%
\subsubsection{Dataset preparation}\label{dataset-preparation}}

We set 5 vegetation classes: dwarf pine, dwarf bamboo, other vegetation, no vegetation, and sky. Using a free image annotation software (Semantic Segmentation Editor, Hitachi, \url{https://github.com/Hitachi-Automotive-And-Industry-Lab/semantic-segmentation-editor}), an expert drew polygons on an image of 2015 for each class. Since vegetation might be different in 2012, 2015, and 2020, applying this teacher data to the image of 2012 and 2020 may cause misclassification. However, vegetation changes are expected to occur at the edge of the plant community. Thus we shrink each polygon with several pixels before applying it to the training.

\hypertarget{implementation-and-model-training}{%
\subsubsection{Implementation and model training}\label{implementation-and-model-training}}

We implemented the classifier with Python3 language and PyTorch deep neural network library (\url{https://pytorch.org/}). All source codes are publically available via GitHub (\url{https://github.com/0kam/xxxx}).

\hypertarget{automatic-georectification}{%
\section{Automatic georectification}\label{automatic-georectification}}

Then, we developed a novel method to convert ground-based landscape imagery into GIS-ready geographical data. This process is called georectification. Georectification of ground-based images has been a difficult task, and this causes the underuse of potentially rich information in ground-based imagery. In plain words, georectification means aligning images into Digital Surface Models (DSMs) so that every pixel of an image gets a geographical coordinate. We can consider a camera as a function that transforms 3D geographical coordinates (latitude, longitude, height) into 2D image coordinates (locations of each pixel). So estimating the parameters of this function (such as the camera location, pose, and the field of view), we can map an image onto a DSM. We recommend readers to \cite{Portenier2020Cryosphere} for a plain mathmatical explanation of this process. Usually, georectification has three steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Finding Ground Control Points (GCPs) in the image.\\
\item
  Estimating camera parameters such as camera poses and field of view using GCPs.\\
\item
  Mapping the image onto the DSM using camera parameters.
\end{enumerate}

Recently, researchers have developed some georectification methods to use ground-based photographs in glaciology (\cite{Messerli2015GeoInst}) and snow cover studies (\cite{Portenier2020Cryosphere}). Especially, \cite{Portenier2020Cryosphere} is worth mentioning for its semi-automatic method using mountain silhouettes as GCPs. However, this silhouette-based method has a drawback in the projection accuracy. It only uses limited areas (silhouettes) of images in the image-to-DSM alignment, and also it ignores lens distortion. Because our target site has a complex vegetation distribution and our camera has considerable lens distortion, we needed a more accurate method.

\hypertarget{local-feature-based-matching-of-images-and-dsms}{%
\subsection{Local-feature-based matching of images and DSMs}\label{local-feature-based-matching-of-images-and-dsms}}

To get matching points between images and DSMs on a broader area, we used an airborne image already georectified. Combining an airborne image and a DSM, we rendered simulated landscape images (Fig. x). Then we got matching points (GCPs) by applying AKAZE local feature matcher to a target image and this simulated image (Fig. x). GCPs have geographical coordinates (from the DSM) and image coordinates (from the image). Our procedure requires the camera's exact location and initial camera parameters to render the simulated image.

\hypertarget{modeling-and-estimating-lens-distortions}{%
\subsection{Modeling and estimating lens distortions}\label{modeling-and-estimating-lens-distortions}}

We modeled the lens distortion (\ref{camera_model}) based on \cite{Weng1992CameraCalib} and OpenCV's implementation, where \({x}''\) and \({y}''\) are the coordinates of each pixels after distortion and \(x'\) and \(y'\) are the coordinates before distortion. Our model includes radial (\(k1~k6\)), tangental (\(p1, p2\)), thin prism (\(s1~s4\)) distortion, and unequal pixel aspect ratio (\(a1, a2\)). See Weng et al.~1992 for the details of lens distortion modeling. Thus, now we can project GCPs' geographical coordinates into image coordinates using lens distortion parameters and other camera parameters (the camera location, pose, and the field of view). We optimized these (except camera location) by minimizing the square projection error of the GCPs using the Covariance Matrix Adaptation Evolution Strategy (CMA-ES, \cite{Hansen2003CMAES}). We could not estimate the camera location because it makes the problem too complicated (e.g., a telephoto taken from a distance and a wide-angle taken from a close look similar).

\begin{equation}
\label{camera_model}
\begin{bmatrix}
{x}'' \\ 
{y}'' 
\end{bmatrix} 
= 
\begin{bmatrix} 
x’ \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + 2 p_1 x’ y’ + p_2(r^2 + 2 x’^2) + s_1 r^2 + s_2 r^4 \\ 
y’ \frac{1 + a_1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + a_2 + k_4 r^2 + k_5 r^4 + k_6 r^6} + p_1 (r^2 + 2 y’^2) + 2 p_2 x’ y’ + s_3 r^2 + s_4 r^4 \ \end{bmatrix}
\end{equation}

\hypertarget{implementation-and-data-set}{%
\subsection{Implementation and data set}\label{implementation-and-data-set}}

We implemented the algorithm with Python3 language and published it as an open-source package via GitHub (\url{https://github.com/0kam/alproj}). You can try it with your data. We used an airborne photograph taken in the November of 201X with a spatial resolution of 1.0 m. Also, we used the 5m resolution Digital Elevation Model provided by the Geospatial Information Authority of Japan (GSI) as a DSM.

\hypertarget{results}{%
\section{Results}\label{results}}

\hypertarget{vegetation-classification-accuracy}{%
\subsection{Vegetation classification accuracy}\label{vegetation-classification-accuracy}}

We evaluated the performance of the vegetation classifier using a 5-fold cross-validation design. Each fold was stratified with the vegetation categories. We used three standard metrics in machine learning evaluation: precision, recall, and F1-score (\ref{metrics}), where \(TP, FP, FN\) is true positives, false positives, and false negatives, respectively.

\begin{align}
\label{metrics}
precision = \frac{TP}{TP + FP} \\
recall = \frac{TP}{TP + FN} \\
F_1 = \frac{2 \cdot precision \cdot recall}{precision + recall} \\
\end{align}

The vegetation classifier achieved a high recall and precision in all five categories \ref{tab:vegetation_cv}. Recall and precision were lowest in dwarf bamboo with the smallest training data. Even in dwarf bamboo, false positives occurred at low frequency (about 5\%), and only a tiny amount of true positives were missed by the pipeline (about 4\%). Since the weighted average of each metric was higher (more than 0.99), balancing the training dataset with vegetation categories may improve these results.

\ref{fig:vegetation} shows the products of vegetation classification. We can observe the distribution of dwarf pine and dwarf bamboo at the plant-community scale.

\begin{table}

\caption{\label{tab:unnamed-chunk-1}Cross validation result \label{tab:vegetation_cv}}
\centering
\begin{tabular}[t]{llll}
\toprule
vegetation & f1-score & precision & recall\\
\midrule
 & 0.982 & 0.977 & 0.987\\
\cmidrule{2-4}
\multirow{-2}{*}{\raggedright\arraybackslash Macro Average} & (0.005) & (0.008) & (0.001)\\
\cmidrule{1-4}
 & 0.997 & 0.997 & 0.997\\
\cmidrule{2-4}
\multirow{-2}{*}{\raggedright\arraybackslash Weighted Average} & (0.001) & (0.001) & (0.001)\\
\cmidrule{1-4}
 & 0.986 & 0.977 & 0.995\\
\cmidrule{2-4}
\multirow{-2}{*}{\raggedright\arraybackslash Dwarf Pine} & (0.005) & (0.012) & (0.003)\\
\cmidrule{1-4}
 & 0.955 & 0.951 & 0.960\\
\cmidrule{2-4}
\multirow{-2}{*}{\raggedright\arraybackslash Dwarf Bamboo} & (0.004) & (0.013) & (0.006)\\
\cmidrule{1-4}
 & 0.997 & 0.999 & 0.996\\
\cmidrule{2-4}
\multirow{-2}{*}{\raggedright\arraybackslash Other vegetation} & (0.001) & (0.000) & (0.002)\\
\cmidrule{1-4}
 & 0.970 & 0.959 & 0.984\\
\cmidrule{2-4}
\multirow{-2}{*}{\raggedright\arraybackslash Non Vegetation} & (0.034) & (0.066) & (0.004)\\
\cmidrule{1-4}
 & 1.000 & 1.000 & 1.000\\
\cmidrule{2-4}
\multirow{-2}{*}{\raggedright\arraybackslash Sky} & (0.000) & (0.000) & (0.000)\\
\bottomrule
\multicolumn{4}{l}{\rule{0pt}{1em}\textit{Note: }}\\
\multicolumn{4}{l}{\rule{0pt}{1em}Mean (SD) of 5-Fold Cross-Varidation}\\
\end{tabular}
\end{table}

\begin{figure}
\includegraphics[width=1\linewidth]{vegetation} \caption{Vegetation classification results}\label{fig:vegetation}
\end{figure}

\hypertarget{georectification-accuracy}{%
\subsection{Georectification accuracy}\label{georectification-accuracy}}

\hypertarget{vegetation-maps}{%
\subsection{Vegetation maps}\label{vegetation-maps}}

\hypertarget{detection-and-localization-of-vegetation-changes}{%
\subsection{Detection and localization of vegetation changes}\label{detection-and-localization-of-vegetation-changes}}

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

We suggested a fully automated procedure to transform time-lapse imagery into georeferenced vegetation maps at a meager cost. This task is challenging because of 1. the difficulty of classifying vegetation with ordinal digital camera imagery and 2. the difficulty of georectification. We solved these issues by 1. using the temporal information of autumn leaf colors for vegetation classification and 2. developing a novel method for accurate image georectification. Our vegetation classification performance (with an accuracy of 0.87) and georectification methods (with an RMSE of \(rmse\) m) are practical.

\hypertarget{the-benefit-of-using-time-series-imagery-for-vegetation-classification}{%
\subsection{The benefit of using time-series imagery for vegetation classification}\label{the-benefit-of-using-time-series-imagery-for-vegetation-classification}}

One of the shortcomings of ordinal digital cameras is that they only have three bands (Red, Blue, and Green). We made up for this lack of information by using the rich temporal information that time-lapse cameras can obtain. Many plant species have characteristic phenologies, such as flowering and autumn foliages. Observing this requires long-term (such as year-round) monitoring with a high frequency, and digital time-lapse cameras are suitable. This study only focused on the two species (dwarf bamboo and stone pine) that are easy to classify, even with a single image. However, like the previous studies (\cite{Tigges2013RemSenEnv}, \cite{Son2013RemSen}, \cite{Heupel2018PFG}), we expect our method to classify more vegetation (e.g., dwarf deciduous trees and alpine herbaceous plants) using the temporal information of leaf colors.
We also used hourly images for each day. Fig. x shows that this additional information made our model robust against transient noises, such as shadows. Since alpine ecosystems have rough topologies, shadows are a considerable problem. Time-lapse cameras are also beneficial for this reason.

\hypertarget{the-performance-of-the-georectification-method-and-its-limitation.}{%
\subsection{The performance of the georectification method and its limitation.}\label{the-performance-of-the-georectification-method-and-its-limitation.}}

\hypertarget{future-application-and-conclusion}{%
\subsection{Future application and conclusion}\label{future-application-and-conclusion}}

\bibliographystyle{unsrt}
\bibliography{export.bib}


\end{document}
